{
  "name": "@barosell/streamllm",
  "version": "0.1.0",
  "description": "Support streaming LLM responses using rxjs observables",
  "main": "lib/index.js",
  "scripts": {
    "build": "tsc",
    "build:w": "tsc --watch",
    "test": "echo \"Error: no test specified\" && exit 1",
    "test:smoke": "ts-node test/smoke-test.ts",
    "test:debate": "ts-node test/debate.ts"
  },
  "keywords": [
    "LLM",
    "rxjx",
    "Ollama",
    "stream"
  ],
  "author": "Bert Rosell",
  "license": "MIT",
  "devDependencies": {
    "@types/node": "^22.10.2",
    "ts-node": "^10.9.2",
    "typescript": "^5.7.2"
  },
  "dependencies": {
    "openai": "^4.77.0",
    "rxjs": "^7.8.1"
  }
}
